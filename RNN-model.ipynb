{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model for Sentiment Analysis\n",
    "\n",
    "Based on the earlier preprocessing technique I have implemented a very basic RNN model in order to get an intuition and implementational details of RNN using tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the saved numpy models of dataset\n",
    "train = np.load('data/train_set.npz')\n",
    "# reviews in form of their word indices\n",
    "train_X = train['train_X']\n",
    "# sentiment for reviews\n",
    "train_y = train['train_y'].reshape(-1,1)\n",
    "# load numpy aray containing word vectors\n",
    "embed = np.load('data/embedding.npz')['embed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is in this form \n",
    "\n",
    "**train_x**: (25000, seq_length)\n",
    "```\n",
    "[\n",
    "[2, 435, 23, 34, 234, 324, 0,  0],\n",
    "[1, 2,   43, 67, 23 , 20,  21, 0]\n",
    "[3, 4,   2,  4,  6,   234, 45, 324],\n",
    "]\n",
    "```\n",
    "\n",
    "**train_y**: (25000, 1)\n",
    "```\n",
    "[1, 1, 0, 0, 1]\n",
    "```\n",
    "\n",
    "**embed**: (voabulary_size, 200)\n",
    "```\n",
    "[\n",
    "[0.06, ................. 0.04],\n",
    "[0.1, 0.23 ....... 0.01, 0.06],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoding of class labels\n",
    "train_y = np.concatenate([train_y, 1-train_y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify we use 2 nodes in output layer thus the train_y array is expanded\n",
    "```\n",
    "[[1,0],\n",
    " [1,0],\n",
    " [0,1],\n",
    " [1,0]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# length of the sequences\n",
    "seq_length = train_X.shape[1]\n",
    "# size of each training batch\n",
    "batch_size = 10\n",
    "# number of iterations in training\n",
    "epochs = 1000\n",
    "# learning rate of optimizer\n",
    "learning_rate = 0.1\n",
    "# shape of input layer\n",
    "input_shape = 200\n",
    "# shape of hidden layer\n",
    "hidden_shape = 256\n",
    "# shape of output layer\n",
    "output_shape = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create mini batches for training\n",
    "def get_batch(x, y, batch_size, random=False, start=0, end=0):\n",
    "    if random:\n",
    "        # shuffled indices of batch_size\n",
    "        idx = np.random.choice(range(len(x)), size=batch_size, replace=False)\n",
    "    else:\n",
    "        # unshuffled indices of batch_size\n",
    "        idx = np.arange(start, end)\n",
    "    return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:02<34:28,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [04:01<36:56,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [08:00<29:27,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.93933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [12:00<25:25,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.04647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [16:06<24:35,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.52397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 501/1000 [20:14<20:50,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.14904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [24:34<16:39,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.77803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [28:43<12:59,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.77753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 801/1000 [32:32<07:21,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 901/1000 [36:23<04:01,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.68995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [40:20<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as graph:\n",
    "    \n",
    "    # parameters for RNN\n",
    "    W_xh = tf.Variable(\n",
    "                        tf.random_normal(mean=0.0, stddev=0.1, shape=[input_shape, hidden_shape]),\n",
    "                        dtype=tf.float32\n",
    "                    )\n",
    "    \n",
    "    W_hh = tf.Variable(\n",
    "                        tf.random_normal(mean=0.0, stddev=0.1, shape=[hidden_shape, hidden_shape]),\n",
    "                        dtype=tf.float32\n",
    "                    )\n",
    "    \n",
    "    W_yh = tf.Variable(\n",
    "                        tf.random_normal(mean=0.0, stddev=0.1, shape=[hidden_shape, output_shape]),\n",
    "                        dtype=tf.float32\n",
    "                    )\n",
    "    \n",
    "    # TODO : add bias\n",
    "    \n",
    "    # embedding tensors which contains word vectors\n",
    "    embeds = tf.constant(embed, dtype=tf.float32)\n",
    "    \n",
    "    # placeholder for inputs (inputs are indices of words in sequences)\n",
    "    X = tf.placeholder(shape=[None, seq_length], dtype=tf.int32)\n",
    "    Y = tf.placeholder(shape=[None, output_shape], dtype=tf.float32)\n",
    "    \n",
    "    # placeholder for initial hidden state\n",
    "    h_in = tf.placeholder(shape=[None, hidden_shape], dtype=tf.float32)\n",
    "    \n",
    "    # inputs in form of vectors (inputs are word vectors for words in sequences)\n",
    "    X_embed = tf.nn.embedding_lookup(embeds, X)\n",
    "    \n",
    "    # the dimension of embedding vector is [batch_size, sequence_length, input_shape]\n",
    "    # we need to convert it into [sequence_length, batch_size, input_shape]\n",
    "    # this is done in order to perform the time unfolding of the RNN graph which is performed by tf.scan() below\n",
    "    X_embed = tf.transpose(X_embed, [1, 0, 2])\n",
    "    \n",
    "    \n",
    "    # define RNN\n",
    "    def RNN(h_prev, x_t):\n",
    "        # reshape input batch of words\n",
    "        x_t = tf.reshape(x_t, [batch_size, input_shape])\n",
    "        \n",
    "        z_t = tf.matmul(x_t, W_xh) + tf.matmul(h_prev, W_hh)\n",
    "        h_t = tf.tanh(z_t)\n",
    "        \n",
    "        # reshape the hidden state output \n",
    "        h_t = tf.reshape(h_t, [batch_size, hidden_shape])\n",
    "        return h_t\n",
    "    \n",
    "    \n",
    "    # tf.scan() function is used to perform an iterative RNN operation over the time steps\n",
    "    # in this case the time unfolding of RNN is done upto sequence length\n",
    "    # tf.scan() function performs RNN operation on a every value of X_embed\n",
    "    # by using the returned value from the previous time step which is the hidden state here\n",
    "    # the final output is the hidden state for every time step\n",
    "    h_out = tf.scan(RNN, X_embed, initializer=h_in)\n",
    "    \n",
    "    # we only need take the hidden state output from the last time step h_out[-1]\n",
    "    # this is fed into a softmax layer\n",
    "    preds = tf.matmul(h_out[-1], W_yh)\n",
    "    \n",
    "    # mean square loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=preds))\n",
    "    #loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, Y))))\n",
    "    \n",
    "    # Adam optimizer for backpropagation\n",
    "    optimize_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # initialize hidden state\n",
    "        h_init = np.zeros((batch_size, hidden_shape))\n",
    "        \n",
    "        # iterate\n",
    "        for i in trange(epochs):\n",
    "            \n",
    "            # generate batches randomly form the train set\n",
    "            batch_X, batch_y = get_batch(train_X, train_y, batch_size, random=True)\n",
    "            \n",
    "            # run the graph for optimize_op and calculate loss\n",
    "            _, cost, pred = sess.run([optimize_op, loss, preds], {X:batch_X, Y:batch_y, h_in:h_init})\n",
    "            \n",
    "            if i%100==0:\n",
    "                print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
