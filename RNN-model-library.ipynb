{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model using library function for Sentiment Analysis\n",
    "\n",
    "Based on the earlier preprocessing technique I have implemented a very basic RNN model in order to get an intuition and implementational details of RNN using tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the saved numpy models of dataset\n",
    "train = np.load('data/train_set.npz')\n",
    "# reviews in form of their word indices\n",
    "train_X = train['train_X']\n",
    "# sentiment for reviews\n",
    "train_y = train['train_y'].reshape(-1,1)\n",
    "# load numpy aray containing word vectors\n",
    "embed = np.load('data/embedding.npz')['embed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is in this form \n",
    "\n",
    "**train_x**: (25000, seq_length)\n",
    "```\n",
    "[\n",
    "[2, 435, 23, 34, 234, 324, 0,  0],\n",
    "[1, 2,   43, 67, 23 , 20,  21, 0]\n",
    "[3, 4,   2,  4,  6,   234, 45, 324],\n",
    "]\n",
    "```\n",
    "\n",
    "**train_y**: (25000, 1)\n",
    "```\n",
    "[1, 1, 0, 0, 1]\n",
    "```\n",
    "\n",
    "**embed**: (voabulary_size, 200)\n",
    "```\n",
    "[\n",
    "[0.06, ................. 0.04],\n",
    "[0.1, 0.23 ....... 0.01, 0.06],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoding of class labels\n",
    "train_y = np.concatenate([train_y, 1-train_y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify we use 2 nodes in output layer thus the train_y array is expanded\n",
    "```\n",
    "[[1,0],\n",
    " [1,0],\n",
    " [0,1],\n",
    " [1,0]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# length of the sequences\n",
    "seq_length = train_X.shape[1]\n",
    "# size of each training batch\n",
    "batch_size = 10\n",
    "# number of iterations in training\n",
    "epochs = 10\n",
    "# learning rate of optimizer\n",
    "learning_rate = 0.1\n",
    "# shape of input layer\n",
    "input_shape = 200\n",
    "# shape of hidden layer\n",
    "hidden_shape = 256\n",
    "# shape of output layer\n",
    "output_shape = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create mini batches for training\n",
    "def get_batch(x, y, batch_size, random=False, start=0, end=0):\n",
    "    if random:\n",
    "        # shuffled indices of batch_size\n",
    "        idx = np.random.choice(range(len(x)), size=batch_size, replace=False)\n",
    "    else:\n",
    "        # unshuffled indices of batch_size\n",
    "        idx = np.arange(start, end)\n",
    "    return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:04<00:36,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as graph:\n",
    "    \n",
    "    # embedding tensors which contains word vectors\n",
    "    embeds = tf.constant(embed, dtype=tf.float32, name=\"embeds\")\n",
    "    \n",
    "    # placeholder for inputs (inputs are indices of words in sequences)\n",
    "    X = tf.placeholder(shape=[None, seq_length], dtype=tf.int32, name=\"input\")\n",
    "    Y = tf.placeholder(shape=[None, output_shape], dtype=tf.float32, name=\"label\")\n",
    "    \n",
    "    # placeholder for initial hidden state\n",
    "    h_in = tf.placeholder(shape=[None, hidden_shape], dtype=tf.float32, name=\"h_in\")\n",
    "    \n",
    "    # inputs in form of vectors (inputs are word vectors for words in sequences)\n",
    "    X_embed = tf.nn.embedding_lookup(embeds, X)\n",
    "    \n",
    "    # the dimension of embedding vector is [batch_size, sequence_length, input_shape]\n",
    "    # we need to convert it into [sequence_length, batch_size, input_shape]\n",
    "    # this is done in order to perform the time unfolding of the RNN graph which is performed by tf.scan() below\n",
    "    #X_embed = tf.transpose(X_embed, [1, 0, 2])\n",
    "    \n",
    "    \n",
    "    # define RNN\n",
    "    def RNN(inputs):\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            cell = tf.contrib.rnn.BasicRNNCell(hidden_shape)\n",
    "            o,s = tf.nn.dynamic_rnn(cell=cell, inputs=inputs,dtype=tf.float32)\n",
    "            return s\n",
    "    \n",
    "    \n",
    "    # we only need take the hidden state output from the last time step h_out[-1]\n",
    "    # this is fed into a softmax layer\n",
    "    with tf.name_scope('predictions'):\n",
    "        W = tf.get_variable(name=\"W\", shape=[hidden_shape, output_shape], dtype=tf.float32)\n",
    "        y = RNN(X_embed)\n",
    "        preds = tf.matmul(y, W)\n",
    "    \n",
    "    # mean square loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=preds))\n",
    "        #loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, Y))))\n",
    "    \n",
    "    # Adam optimizer for backpropagation\n",
    "    with tf.name_scope('train'):\n",
    "        optimize_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # initialize hidden state\n",
    "        h_init = np.zeros((batch_size, hidden_shape))\n",
    "        \n",
    "        # iterate\n",
    "        for i in trange(epochs):\n",
    "            \n",
    "            # generate batches randomly form the train set\n",
    "            batch_X, batch_y = get_batch(train_X[:100], train_y[:100], batch_size, random=True)\n",
    "            \n",
    "            # run the graph for optimize_op and calculate loss\n",
    "            _, cost, pred = sess.run([optimize_op, loss, preds], {X:batch_X, Y:batch_y, h_in:h_init})\n",
    "            \n",
    "            if i%10==0:\n",
    "                print(cost)\n",
    "        \n",
    "        writer = tf.summary.FileWriter('./tmp/1')\n",
    "        writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TensorBoard b'41' on port 6006\n",
      "(You can navigate to http://127.0.1.1:6006)\n",
      "WARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "^CTraceback (most recent call last):\n",
      "  File \"/home/shivam/anaconda3/bin/tensorboard\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/shivam/anaconda3/lib/python3.5/site-packages/tensorflow/tensorboard/tensorboard.py\", line 151, in main\n",
      "    tb_server.serve_forever()\n",
      "  File \"/home/shivam/anaconda3/lib/python3.5/socketserver.py\", line 232, in serve_forever\n",
      "    ready = selector.select(poll_interval)\n",
      "  File \"/home/shivam/anaconda3/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir ./tmp/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
