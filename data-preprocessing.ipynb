{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook describes in detail the steps involved in preprocessing the dataset for Sentiment Classification problem. These methods can be generalized for use in most of NLP tasks. The methods used are quite naive and can be considerably improved, this is just a beginner's introduction.\n",
    "\n",
    "Let's dive into the process staright away..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies\n",
    "\n",
    "**pandas** : to read the dataset\n",
    "\n",
    "\n",
    "**numpy** : to help us with numerical computation \n",
    "\n",
    "\n",
    "**string** : to help with cleaning \n",
    "\n",
    "\n",
    "**re** : to find regular expression \n",
    "\n",
    "\n",
    "**tqdm** : to make iterations less boring\n",
    "\n",
    "\n",
    "**collections** : to do some counting\n",
    "\n",
    "\n",
    "**gensim** : to help with making word embedding vectors\n",
    "\n",
    "\n",
    "**multiprocessing** : to get the count of number of available CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set to look into our dataset.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('resources/labeledTrainData.tsv', sep='\\t')\n",
    "print(\"shape: \",data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 3 columns and 25000 samples\n",
    "\n",
    "**id** : it is just the question id and is irrelevant as of now\n",
    "\n",
    "**sentiment** : contains the positive and negative sentiment labels for each review\n",
    "\n",
    "**review** : movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before I go into preprocessing and all the details let's first have a look at fragment of the reviews\n",
    "```\n",
    "\\\"The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book from 1968.\n",
    "```\n",
    "Inorder to classify this review into positive or negative we need to train a model which can learn differences between the inputs but first of all we need to understand what these inputs will be.\n",
    "\n",
    "If we look at the above sentence we see that the sentence has a lot of noise like \\ \" ' . Then there are cases where 'Classic' and 'classic' are present which may be treated as different words. We also some have less common words like names and years(which are numeric).\n",
    "\n",
    "These all are irrelevant and may have adverse effect to our model. So we solve these issues in following manner by first aggregating all the reviews together and then applying our preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert reviews from dataframe to list\n",
    "sentences = list(data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess sentences\n",
    "def preprocess(sent):\n",
    "    # convert string to lower case, remove punctutaions and replace digits with '#'\n",
    "    sent = ''.join('#' if s.isdigit() else s for s in sent if s not in string.punctuation).lower()\n",
    "    # substitute '####' with '#+' and tokenize\n",
    "    sent = re.sub('#+', '#', sent).split(' ')\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above preprocessing function is applied we get output which is similar to this\n",
    "```\n",
    "['the', 'classic', 'war', 'of', 'the', 'worlds', 'by', 'timothy', 'hines', 'is', 'a', 'very', 'entertaining', 'film', 'that', 'obviously', 'goes', 'to', 'great', 'effort', 'and', 'lengths', 'to', 'faithfully', 'recreate', 'h', 'g', 'wells', 'classic', 'book', 'from', '#']\n",
    "```\n",
    "\n",
    "This is done for all the sentences and hence we get a list like this\n",
    "```\n",
    "[\n",
    "[...],\n",
    "['the', 'classic', 'war', 'of', 'the', 'worlds', 'by', 'timothy', 'hines', 'is', 'a', 'very', 'entertaining', 'film', 'that', 'obviously', 'goes', 'to', 'great', 'effort', 'and', 'lengths', 'to', 'faithfully', 'recreate', 'h', 'g', 'wells', 'classic', 'book', 'from', '#'],\n",
    "[...],\n",
    "]\n",
    "```\n",
    "\n",
    "Now we need to create a vocabulary of all the words present in all the sentences. But we need to keep in mind that we do not include those words in our vocabulary which infrequent like person names 'timothy', 'hines'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create vocabulary for the given dataset\n",
    "def create_vocab(sent):\n",
    "    # concats all the words in all the sentences of the dataset into one single list\n",
    "    words = []\n",
    "    for s in sent:\n",
    "        words.extend(s)\n",
    "    # count the occurence of each word in the dataset(words list)\n",
    "    word_count = Counter(words)\n",
    "    # create vocabulary with words occuring more than 2 times\n",
    "    global vocab\n",
    "    vocab = set([k for k,v in word_count.items() if v>2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us just suppose for our above sentence that we create a vocabulary which maybe like this\n",
    "```\n",
    "['the', 'classic', 'war', 'of', 'worlds', 'by', 'is', 'a', 'very', 'entertaining', 'film', 'that', 'obviously', 'goes', 'to', 'great', 'effort', 'and', 'lengths', 'faithfully', 'recreate', 'book', 'from', '#']\n",
    "```\n",
    "Note that the names which may not be frequent in all reviews are removed for explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove words not present in the vocabulary\n",
    "def remove_unknown(sent):\n",
    "    sent = [w if w in vocab else 'unk' for w in sent]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to replace the words in a sentence which is not present in vocabulary by 'unk' tag. The output for example sequence and vocabulary will be:\n",
    "```\n",
    "['the', 'classic', 'war', 'of', 'the', 'worlds', 'by', 'unk', 'unk', 'is', 'a', 'very', 'entertaining', 'film', 'that', 'obviously', 'goes', 'to', 'great', 'effort', 'and', 'lengths', 'to', 'faithfully', 'recreate', 'unk', 'unk', 'unk', 'classic', 'book', 'from', '#']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2665.88it/s]\n",
      "100%|██████████| 25000/25000 [00:01<00:00, 22155.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocess sentences - tokenization, removing punctuations and digits\n",
    "sentences = [preprocess(sent) for sent in tqdm(sentences)]\n",
    "# create vocab\n",
    "create_vocab(sentences)\n",
    "# replace unkown words with 'unk' tag\n",
    "sentences = [remove_unknown(sent) for sent in tqdm(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "All the sentences are now in the following manner\n",
    "```\n",
    "[\n",
    "[...],\n",
    "['the', 'classic', 'war', 'of', 'the', 'worlds', 'by', 'unk', 'unk', 'is', 'a', 'very', 'entertaining', 'film', 'that', 'obviously', 'goes', 'to', 'great', 'effort', 'and', 'lengths', 'to', 'faithfully', 'recreate', 'unk', 'unk', 'unk', 'classic', 'book', 'from', '#'],\n",
    "[...],\n",
    "]\n",
    "```\n",
    "\n",
    "We feed-in these sentences into a word2vec model to generate word embedding vectors for each of the words present. These word vectors are able to successfully capture the semantic relations between each words in their neighborhood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set train_word2vec = True to train the word embeddings using skip-gram model\n",
    "train_word2vec = False\n",
    "\n",
    "if train_word2vec:\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    model = Word2Vec(sentences, min_count=1, size=200, sg=1, iter=2, negative=10, workers=cores)\n",
    "    model.save('./resources/word2vec/word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained we have a dictionary of word and their vectors like this:\n",
    "```\n",
    "{\n",
    "    'classic' : [0.06, ................. 0.04],\n",
    "    'the'     : [0.1, 0.23 ....... 0.01, 0.06],\n",
    "}\n",
    "```\n",
    "\n",
    "Now we create two dictionaries word-to-index and index-to-word which help us in the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# index to word dictionary\n",
    "id_to_word = dict(enumerate(list(vocab) + ['unk']))\n",
    "\n",
    "# word to index dictionary\n",
    "word_to_id = {v: k for k, v in id_to_word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These dictionaries look somewhat like this\n",
    "```\n",
    "id_to_word = {  'classic' : 1, 'the' : 2, ....., 'from' : 20, '#' : 21  }\n",
    "\n",
    "word_to_id = {  1 : 'classic', 2 : 'the', ....., 20 : 'from', 21 : '#' }\n",
    "```\n",
    "Now we need to convert all the words to their indices in all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change list of tokenized words to word indices for every sentences\n",
    "sentences = [[word_to_id[w] for w in sent] for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words are replced by their corresponding indices as mentioned in the word_to_index dictionary. After this step the list of all reviews look like this.\n",
    "```\n",
    "[\n",
    "[2, 435, 23, 34, 234, 324],\n",
    "[1, 2, 43, 67, 23 , 20, 21]\n",
    "[3, 4, 2, 4, 6, 234 , 45, 324],\n",
    "]\n",
    "```\n",
    "\n",
    "since the sentences are of variable size like in the above case sentences are of sizes 6, 7 and 8 we need to make them of equal length by padding extra indices are the end. These indices can be any number but conventionally we use 0's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find max length sequence\n",
    "max_length = max([len(sent) for sent in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:00<00:00, 43499.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# pad 0's to the end of each sequence\n",
    "for i in trange(len(sentences)):\n",
    "    sentences[i] = sentences[i] + [0]*(max_length - len(sentences[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the padding the sequences becomes\n",
    "```\n",
    "[\n",
    "[2, 435, 23, 34, 234, 324, 0,  0],\n",
    "[1, 2,   43, 67, 23 , 20,  21, 0]\n",
    "[3, 4,   2,  4,  6,   234, 45, 324],\n",
    "]\n",
    "```\n",
    "\n",
    "For further usage it is better to convert the dictionary of word2vec into an array as it makes it faster to access when performing operations in tensorflow and uses less memory. \n",
    "\n",
    "[TODO: explain why]\n",
    "\n",
    "We need to convert this dictionary\n",
    "```\n",
    "{\n",
    "    'classic' : [0.06, ................. 0.04],\n",
    "    'the'     : [0.1, 0.23 ....... 0.01, 0.06],\n",
    "}\n",
    "```\n",
    "into an array like\n",
    "```\n",
    "[\n",
    "[0.06, ................. 0.04],        // this row index is 1 which contains word vector for 'classic' : 1\n",
    "[0.1, 0.23 ....... 0.01, 0.06],        // this row index is 2 which contains word vector for 'the' : 2\n",
    "]\n",
    "```\n",
    "and to access the vectors from the array we can use the indices of each word from this\n",
    "```\n",
    "id_to_word = {  'classic' : 1, 'the' : 2, ....., 'from' : 20, '#' : 21  }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initalize an embeddings array which contains word vectors for all the words in vocabulary. These word vectors can \n",
    "# be accesed by the indices from this vector\n",
    "embed = np.zeros((vocab_size+1, 200))\n",
    "for k,v in word_to_id.items():\n",
    "    embed[v] = model[k]\n",
    "# assigning 0's vector of size 200 for padding\n",
    "# note: 0 index in vocabulary was assigned to empty word ''\n",
    "embed[0] = np.zeros((200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13443,\n",
       " 34047,\n",
       " 8236,\n",
       " 8362,\n",
       " 27533,\n",
       " 33631,\n",
       " 12734,\n",
       " 17840,\n",
       " 24362,\n",
       " 13443,\n",
       " 5624,\n",
       " 36099,\n",
       " 3685,\n",
       " 29223,\n",
       " 14051,\n",
       " 25130,\n",
       " 14368,\n",
       " 37118,\n",
       " 17840,\n",
       " 13107,\n",
       " 13239,\n",
       " 35653,\n",
       " 37951,\n",
       " 33962,\n",
       " 12047,\n",
       " 17840,\n",
       " 13133,\n",
       " 37951,\n",
       " 12047,\n",
       " 9630,\n",
       " 19710,\n",
       " 38278,\n",
       " 4168,\n",
       " 21888,\n",
       " 27124,\n",
       " 14051,\n",
       " 36470,\n",
       " 13676,\n",
       " 16445,\n",
       " 20452,\n",
       " 4604,\n",
       " 8236,\n",
       " 15731,\n",
       " 17894,\n",
       " 4168,\n",
       " 882,\n",
       " 3169,\n",
       " 16916,\n",
       " 725,\n",
       " 37668,\n",
       " 17840,\n",
       " 12040,\n",
       " 21888,\n",
       " 14051,\n",
       " 38278,\n",
       " 24054,\n",
       " 36285,\n",
       " 23112,\n",
       " 9833,\n",
       " 22100,\n",
       " 10560,\n",
       " 13122,\n",
       " 25810,\n",
       " 3876,\n",
       " 19377,\n",
       " 9630,\n",
       " 13122,\n",
       " 16503,\n",
       " 23771,\n",
       " 16503,\n",
       " 8203,\n",
       " 37780,\n",
       " 22887,\n",
       " 4168,\n",
       " 16116,\n",
       " 27533,\n",
       " 14051,\n",
       " 11159,\n",
       " 12734,\n",
       " 17840,\n",
       " 1400,\n",
       " 10974,\n",
       " 41729,\n",
       " 3169,\n",
       " 16584,\n",
       " 35255,\n",
       " 22043,\n",
       " 12638,\n",
       " 41729,\n",
       " 29717,\n",
       " 17101,\n",
       " 33896,\n",
       " 8177,\n",
       " 501,\n",
       " 16029,\n",
       " 31208,\n",
       " 17840,\n",
       " 13377,\n",
       " 37951,\n",
       " 6917,\n",
       " 17840,\n",
       " 35965,\n",
       " 42968,\n",
       " 12638,\n",
       " 18584,\n",
       " 40933,\n",
       " 38315,\n",
       " 43232,\n",
       " 5368,\n",
       " 5920,\n",
       " 31017,\n",
       " 9330,\n",
       " 12638,\n",
       " 35309,\n",
       " 8236,\n",
       " 13122,\n",
       " 34047,\n",
       " 8177,\n",
       " 16918,\n",
       " 24797,\n",
       " 38192,\n",
       " 17065,\n",
       " 39203,\n",
       " 12314,\n",
       " 2218,\n",
       " 5624,\n",
       " 37668,\n",
       " 29332,\n",
       " 10083,\n",
       " 39203,\n",
       " 40933,\n",
       " 27533,\n",
       " 14051,\n",
       " 14271,\n",
       " 8236,\n",
       " 37951,\n",
       " 39601,\n",
       " 41729,\n",
       " 27387,\n",
       " 22043,\n",
       " 3274,\n",
       " 1620,\n",
       " 5624,\n",
       " 14890,\n",
       " 38073,\n",
       " 38122,\n",
       " 31249,\n",
       " 14051,\n",
       " 17840,\n",
       " 32394,\n",
       " 12638,\n",
       " 8236,\n",
       " 19987,\n",
       " 9330,\n",
       " 5624,\n",
       " 37951,\n",
       " 6135,\n",
       " 12638,\n",
       " 25130,\n",
       " 17747,\n",
       " 40445,\n",
       " 25795,\n",
       " 10864,\n",
       " 10560,\n",
       " 25765,\n",
       " 41729,\n",
       " 38122,\n",
       " 17840,\n",
       " 17747,\n",
       " 22887,\n",
       " 18639,\n",
       " 40251,\n",
       " 13122,\n",
       " 16916,\n",
       " 14634,\n",
       " 12638,\n",
       " 4257,\n",
       " 5368,\n",
       " 17840,\n",
       " 4805,\n",
       " 8203,\n",
       " 37780,\n",
       " 9369,\n",
       " 10974,\n",
       " 41729,\n",
       " 1649,\n",
       " 24937,\n",
       " 13122,\n",
       " 13649,\n",
       " 32974,\n",
       " 38122,\n",
       " 41722,\n",
       " 12926,\n",
       " 3876,\n",
       " 38192,\n",
       " 9441,\n",
       " 17840,\n",
       " 10588,\n",
       " 19283,\n",
       " 29982,\n",
       " 37951,\n",
       " 27059,\n",
       " 40922,\n",
       " 13122,\n",
       " 39663,\n",
       " 38635,\n",
       " 13676,\n",
       " 18347,\n",
       " 34047,\n",
       " 32729,\n",
       " 8654,\n",
       " 12852,\n",
       " 41799,\n",
       " 10560,\n",
       " 25972,\n",
       " 5624,\n",
       " 1965,\n",
       " 38192,\n",
       " 38315,\n",
       " 13122,\n",
       " 21778,\n",
       " 33523,\n",
       " 29713,\n",
       " 5624,\n",
       " 11234,\n",
       " 25130,\n",
       " 41810,\n",
       " 21208,\n",
       " 27059,\n",
       " 2804,\n",
       " 27783,\n",
       " 23130,\n",
       " 10864,\n",
       " 10560,\n",
       " 11215,\n",
       " 30882,\n",
       " 14051,\n",
       " 31118,\n",
       " 41729,\n",
       " 13122,\n",
       " 10560,\n",
       " 17894,\n",
       " 13122,\n",
       " 9540,\n",
       " 18584,\n",
       " 10835,\n",
       " 38192,\n",
       " 4168,\n",
       " 27748,\n",
       " 38278,\n",
       " 10560,\n",
       " 21888,\n",
       " 38691,\n",
       " 501,\n",
       " 37605,\n",
       " 5368,\n",
       " 7066,\n",
       " 12638,\n",
       " 725,\n",
       " 17427,\n",
       " 37668,\n",
       " 8236,\n",
       " 2218,\n",
       " 5624,\n",
       " 17795,\n",
       " 4604,\n",
       " 13676,\n",
       " 18416,\n",
       " 37951,\n",
       " 13676,\n",
       " 18085,\n",
       " 37951,\n",
       " 17840,\n",
       " 31639,\n",
       " 42664,\n",
       " 33282,\n",
       " 29982,\n",
       " 6917,\n",
       " 17840,\n",
       " 21151,\n",
       " 20691,\n",
       " 77,\n",
       " 16023,\n",
       " 17840,\n",
       " 22868,\n",
       " 12638,\n",
       " 13676,\n",
       " 7715,\n",
       " 10974,\n",
       " 41729,\n",
       " 37084,\n",
       " 14051,\n",
       " 40090,\n",
       " 17840,\n",
       " 1281,\n",
       " 38315,\n",
       " 29982,\n",
       " 38635,\n",
       " 19228,\n",
       " 24327,\n",
       " 14271,\n",
       " 13409,\n",
       " 13443,\n",
       " 32212,\n",
       " 18680,\n",
       " 31458,\n",
       " 6310,\n",
       " 13676,\n",
       " 31639,\n",
       " 37642,\n",
       " 12638,\n",
       " 34498,\n",
       " 42260,\n",
       " 13676,\n",
       " 2086,\n",
       " 19099,\n",
       " 34884,\n",
       " 5368,\n",
       " 22055,\n",
       " 21329,\n",
       " 8236,\n",
       " 19987,\n",
       " 13122,\n",
       " 38122,\n",
       " 30882,\n",
       " 17894,\n",
       " 2218,\n",
       " 5624,\n",
       " 32974,\n",
       " 32212,\n",
       " 34892,\n",
       " 3876,\n",
       " 805,\n",
       " 22887,\n",
       " 4168,\n",
       " 31225,\n",
       " 13122,\n",
       " 6135,\n",
       " 30882,\n",
       " 18639,\n",
       " 41881,\n",
       " 10083,\n",
       " 5032,\n",
       " 12509,\n",
       " 41729,\n",
       " 23411,\n",
       " 13546,\n",
       " 37951,\n",
       " 37089,\n",
       " 11139,\n",
       " 13676,\n",
       " 28563,\n",
       " 42968,\n",
       " 37951,\n",
       " 1978,\n",
       " 501,\n",
       " 43232,\n",
       " 23264,\n",
       " 37668,\n",
       " 8236,\n",
       " 19987,\n",
       " 13122,\n",
       " 13676,\n",
       " 36787,\n",
       " 16918,\n",
       " 24797,\n",
       " 13122,\n",
       " 26879,\n",
       " 32212,\n",
       " 12638,\n",
       " 17840,\n",
       " 6135,\n",
       " 28469,\n",
       " 30882,\n",
       " 7987,\n",
       " 14051,\n",
       " 29198,\n",
       " 8236,\n",
       " 12829,\n",
       " 9330,\n",
       " 13122,\n",
       " 10560,\n",
       " 25810,\n",
       " 36127,\n",
       " 13443,\n",
       " 34047,\n",
       " 17840,\n",
       " 42161,\n",
       " 36099,\n",
       " 11011,\n",
       " 8236,\n",
       " 43232,\n",
       " 36127,\n",
       " 4168,\n",
       " 27857,\n",
       " 31118,\n",
       " 29713,\n",
       " 30882,\n",
       " 39444,\n",
       " 34371,\n",
       " 14549,\n",
       " 25702,\n",
       " 20505,\n",
       " 21171,\n",
       " 4168,\n",
       " 31118,\n",
       " 8236,\n",
       " 38122,\n",
       " 13676,\n",
       " 40323,\n",
       " 10560,\n",
       " 13122,\n",
       " 41944,\n",
       " 14890,\n",
       " 15149,\n",
       " 14634,\n",
       " 9330,\n",
       " 28888,\n",
       " 15731,\n",
       " 3876,\n",
       " 32212,\n",
       " 12638,\n",
       " 17840,\n",
       " 6135,\n",
       " 5723,\n",
       " 17542,\n",
       " 4168,\n",
       " 15184,\n",
       " 10560,\n",
       " 13122,\n",
       " 41881,\n",
       " 17840,\n",
       " 22378,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample sentence now assigned with indices value and padding\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = np.array(sentences)\n",
    "train_y = np.array(data['sentiment'])\n",
    "! mkdir data\n",
    "np.savez('data/train_set.npz', train_X=train_X, train_y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('data/embedding.npz', embed=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
